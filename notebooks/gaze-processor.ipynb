{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Tuple, Callable\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VideoInfo:\n",
    "    width: int\n",
    "    height: int\n",
    "    fps: int\n",
    "\n",
    "@dataclass\n",
    "class ProcessConfig:\n",
    "    input_path: str\n",
    "    video_id: str\n",
    "    timestamps_path: str\n",
    "    max_queue_size: int\n",
    "    output_dir: str\n",
    "    output_video_path: str\n",
    "    output_json_path: str\n",
    "    total_frames: int = 0\n",
    "\n",
    "@dataclass\n",
    "class YOLOConfig:\n",
    "    model_path: str\n",
    "    confidence_threshold: float\n",
    "    target_classes: List[int]\n",
    "    model: Any\n",
    "    timestamps: List[str]\n",
    "\n",
    "@dataclass\n",
    "class YOLOProcessConfig:\n",
    "    # Fields from ProcessConfig\n",
    "    input_path: str\n",
    "    video_id: str\n",
    "    timestamps_path: str\n",
    "    max_queue_size: int\n",
    "    output_dir: str\n",
    "    output_video_path: str\n",
    "    output_json_path: str\n",
    "    total_frames: int = 0\n",
    "    \n",
    "    # YOLO-specific field\n",
    "    yolo_config: YOLOConfig = field(default_factory=dict)\n",
    "\n",
    "class ProgressTracker:\n",
    "    def __init__(self, total, desc):\n",
    "        self.total = total\n",
    "        self.desc = desc\n",
    "        self.lock = threading.Lock()\n",
    "        self.pbar = None\n",
    "\n",
    "    def start(self):\n",
    "        self.pbar = tqdm(total=self.total, desc=self.desc, position=1, leave=False)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        with self.lock:\n",
    "            if self.pbar:\n",
    "                self.pbar.update(n)\n",
    "\n",
    "    def close(self):\n",
    "        if self.pbar:\n",
    "            self.pbar.close()\n",
    "\n",
    "class FileIO:\n",
    "    @staticmethod\n",
    "    def read_lines(file_path: str) -> List[str]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_json(file_path: str) -> Dict[str, Any]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def write_json(file_path: str, data: Dict[str, Any]):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "class VideoProcessor(ABC):\n",
    "    def __init__(self, output_dir: str, default_max_queue_size: int = 30, max_workers: int = None):\n",
    "        self.output_dir = output_dir\n",
    "        self.default_max_queue_size = default_max_queue_size\n",
    "        self.max_workers = max_workers\n",
    "        self.text_reader = FileIO()\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_process_config(self, video_config: Dict[str, Any]) -> Any:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _process_frames(self, frame_queue: Queue, result_queue: Queue, process_config: Any, results_dict: Dict[str, Any], progress_callback: Callable[[int], None]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _post_process(self, result_queue: Queue, output_queue: Queue, progress_callback: Callable[[int], None]):\n",
    "        pass\n",
    "\n",
    "    def process_single_video(self, process_config: Any, pbar: tqdm):\n",
    "        cap, video_info = self._initialize_video_capture(process_config.input_path)\n",
    "        process_config.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        queues = self._create_queues(process_config.max_queue_size)\n",
    "        results_dict: Dict[str, Any] = {}\n",
    "\n",
    "        pbar.total = process_config.total_frames\n",
    "        pbar.set_description(f\"Processing {process_config.video_id}\")\n",
    "        pbar.reset()\n",
    "\n",
    "        def progress_callback(n):\n",
    "            pbar.update(n)\n",
    "\n",
    "        threads = self._create_and_start_threads(cap, queues, process_config, results_dict, video_info, progress_callback)\n",
    "        self._join_threads(threads)\n",
    "\n",
    "        self._save_results(process_config.output_json_path, results_dict)\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def process_videos(self, video_configs: List[Dict[str, Any]]):\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = []\n",
    "            progress_bars = []\n",
    "            \n",
    "            for video_config in video_configs:\n",
    "                config = self.create_process_config(video_config)\n",
    "                pbar = tqdm(total=0, desc=f\"Initializing {config.video_id}\", position=len(progress_bars), leave=True)\n",
    "                progress_bars.append(pbar)\n",
    "                futures.append(executor.submit(self.process_single_video, config, pbar))\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {str(e)}\")\n",
    "            \n",
    "            for pbar in progress_bars:\n",
    "                pbar.close()\n",
    "\n",
    "    def _initialize_video_capture(self, input_path: str) -> Tuple[cv2.VideoCapture, VideoInfo]:\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        video_info = VideoInfo(\n",
    "            width=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            height=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            fps=int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        )\n",
    "        return cap, video_info\n",
    "\n",
    "    def _create_queues(self, max_queue_size: int) -> Dict[str, Queue]:\n",
    "        return {\n",
    "            'frame': Queue(maxsize=max_queue_size),\n",
    "            'result': Queue(maxsize=max_queue_size),\n",
    "            'output': Queue(maxsize=max_queue_size)\n",
    "        }\n",
    "\n",
    "    def _create_and_start_threads(self, cap: cv2.VideoCapture, queues: Dict[str, Queue], \n",
    "                                  process_config: Any, results_dict: Dict[str, Any], \n",
    "                                  video_info: VideoInfo, progress_callback: Callable[[int], None]) -> List[Thread]:\n",
    "        threads = [\n",
    "            Thread(target=self._read_frames, args=(cap, queues['frame'], process_config.max_queue_size)),\n",
    "            Thread(target=self._process_frames, args=(queues['frame'], queues['result'], process_config, results_dict, progress_callback)),\n",
    "            Thread(target=self._post_process, args=(queues['result'], queues['output'], progress_callback)),\n",
    "            Thread(target=self._write_video, args=(process_config.output_video_path, queues['output'], \n",
    "                                                   video_info.fps, video_info.width, video_info.height))\n",
    "        ]\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        return threads\n",
    "\n",
    "\n",
    "    def _join_threads(self, threads: List[Thread]):\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def _read_frames(self, cap: cv2.VideoCapture, frame_queue: Queue, max_queue_size: int):\n",
    "        while True:\n",
    "            if frame_queue.qsize() < max_queue_size:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame_queue.put(frame)\n",
    "            else:\n",
    "                time.sleep(0.1)\n",
    "        frame_queue.put(None)\n",
    "\n",
    "    def _write_video(self, output_path: str, output_queue: Queue, fps: int, width: int, height: int):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        while True:\n",
    "            frame = output_queue.get()\n",
    "            if frame is None:\n",
    "                break\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        out.release()\n",
    "\n",
    "    def _save_results(self, output_json_path: str, results_dict: Dict[str, Any]):\n",
    "        self.text_reader.write_json(output_json_path, results_dict)\n",
    "    \n",
    "    def get_unique_output_dir(self, base_path: str) -> str:\n",
    "        if not os.path.exists(base_path):\n",
    "            return base_path\n",
    "        \n",
    "        counter = 1\n",
    "        while True:\n",
    "            new_path = f\"{base_path}_copy{counter}\"\n",
    "            if not os.path.exists(new_path):\n",
    "                return new_path\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GazeDetectionConfig(YOLOProcessConfig):\n",
    "    num_frames_for_radius: int = 10\n",
    "    output_text_path: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.output_text_path = os.path.join(self.output_dir, f\"{self.video_id}_gaze_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeDetectionProcessor(VideoProcessor):\n",
    "    def __init__(self, output_dir: str, default_max_queue_size: int = 30, max_workers: int = None):\n",
    "        super().__init__(output_dir, default_max_queue_size, max_workers)\n",
    "        self.consistent_radius = None\n",
    "        self.radius_lock = threading.Lock()\n",
    "\n",
    "    def create_process_config(self, video_config: Dict[str, Any]) -> GazeDetectionConfig:\n",
    "        config = GazeDetectionConfig(\n",
    "            input_path=video_config['input_path'],\n",
    "            video_id=video_config['video_id'],\n",
    "            timestamps_path=video_config['timestamps_path'],\n",
    "            max_queue_size=self.default_max_queue_size,\n",
    "            output_dir=self.output_dir,\n",
    "            output_video_path=\"\",  # We won't be using this\n",
    "            output_json_path=\"\",  # We won't be using this\n",
    "            num_frames_for_radius=video_config.get('num_frames_for_radius', 10)\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def _get_consistent_radius(self, cap: cv2.VideoCapture, num_frames: int) -> int:\n",
    "        radii = []\n",
    "        for _ in range(num_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            _, _, radius = self._get_circle_BB(frame)\n",
    "            radii.append(radius)\n",
    "        \n",
    "        # Use the most common radius as the consistent radius\n",
    "        consistent_radius = Counter(radii).most_common(1)[0][0]\n",
    "        print(f\"Consistent gaze circle radius: {consistent_radius}\")\n",
    "        \n",
    "        # Reset video capture to the beginning\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        return consistent_radius\n",
    "\n",
    "    def _get_circle_BB(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray_blurred = cv2.blur(gray, (3, 3))\n",
    "        detected_circles = cv2.HoughCircles(gray_blurred,\n",
    "                        cv2.HOUGH_GRADIENT, 1, 20, param1=50,\n",
    "                    param2=30, minRadius=18, maxRadius=19)\n",
    "\n",
    "        if detected_circles is not None:\n",
    "            detected_circles = np.uint16(np.around(detected_circles))\n",
    "            first_circle = detected_circles[0, :][0]\n",
    "            x_circle, y_circle, r = map(float, first_circle)\n",
    "            return x_circle, y_circle, r\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    def _process_frames(self, frame_queue: Queue, result_queue: Queue, process_config: GazeDetectionConfig, results_dict: Dict[str, Any], progress_callback: Callable[[int], None]):\n",
    "        cap, _ = self._initialize_video_capture(process_config.input_path)\n",
    "        consistent_radius = self._get_consistent_radius(cap, process_config.num_frames_for_radius)\n",
    "        \n",
    "        with self.radius_lock:\n",
    "            self.consistent_radius = consistent_radius\n",
    "        \n",
    "        frame_number = 0\n",
    "        while True:\n",
    "            frame = frame_queue.get()\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            x, y, _ = self._get_circle_BB(frame)\n",
    "            result_queue.put((frame_number, x, y))\n",
    "            \n",
    "            frame_number += 1\n",
    "            progress_callback(1)\n",
    "        \n",
    "        result_queue.put(None)\n",
    "\n",
    "\n",
    "    def _post_process(self, result_queue: Queue, output_queue: Queue, progress_callback: Callable[[int], None]):\n",
    "        results = []\n",
    "        while True:\n",
    "            result = result_queue.get()\n",
    "            if result is None:\n",
    "                break\n",
    "            \n",
    "            frame_number, x, y = result\n",
    "            results.append((frame_number, x, y))\n",
    "            progress_callback(1)\n",
    "        \n",
    "        output_queue.put(results)\n",
    "        output_queue.put(None)\n",
    "\n",
    "    def _write_text_output(self, output_path: str, consistent_radius: float, results: List[Tuple[int, float, float]]):\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"{consistent_radius}\\n\")\n",
    "            for frame_number, x, y in results:\n",
    "                f.write(f\"{frame_number},{x},{y}\\n\")\n",
    "\n",
    "    def process_single_video(self, process_config: GazeDetectionConfig, pbar: tqdm):\n",
    "        cap, video_info = self._initialize_video_capture(process_config.input_path)\n",
    "        process_config.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        queues = self._create_queues(process_config.max_queue_size)\n",
    "        results_dict: Dict[str, Any] = {}\n",
    "\n",
    "        pbar.total = process_config.total_frames\n",
    "        pbar.set_description(f\"Processing {process_config.video_id}\")\n",
    "        pbar.reset()\n",
    "\n",
    "        def progress_callback(n):\n",
    "            pbar.update(n)\n",
    "\n",
    "        threads = self._create_and_start_threads(cap, queues, process_config, results_dict, video_info, progress_callback)\n",
    "        \n",
    "        # Wait for processing to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        # Get the results from the output queue\n",
    "        results = queues['output'].get()\n",
    "\n",
    "        # Write the text output\n",
    "        with self.radius_lock:\n",
    "            self._write_text_output(process_config.output_text_path, self.consistent_radius, results)\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "    def _create_and_start_threads(self, cap: cv2.VideoCapture, queues: Dict[str, Queue], \n",
    "                                  process_config: GazeDetectionConfig, results_dict: Dict[str, Any], \n",
    "                                  video_info: VideoInfo, progress_callback: Callable[[int], None]) -> List[Thread]:\n",
    "        threads = [\n",
    "            Thread(target=self._read_frames, args=(cap, queues['frame'], process_config.max_queue_size)),\n",
    "            Thread(target=self._process_frames, args=(queues['frame'], queues['result'], process_config, results_dict, progress_callback)),\n",
    "            Thread(target=self._post_process, args=(queues['result'], queues['output'], progress_callback)),\n",
    "        ]\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        return threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VIDEO_PATH = \"data/videos/Rec16-1_trimmed.mp4\"\n",
    "# OUTPUT_VIDEO_PATH = \"output/videos/Rec16-1-yolo_trimmed_final.mp4\"\n",
    "INPUT_TIMESTAMP_PATH = \"output/timestamps/Rec16-1_trimmed.txt\"\n",
    "# JSON_OUTPUT_PATH = \"output/json/Rec16-1_trimmed_yolo_final.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "YOLO_MODEL_PATH = \"models/yolo/yolov8n.pt\"\n",
    "YOLO_CONFIDENCE_THRESHOLD = 0.5\n",
    "MAX_QUEUE_SIZE = 30\n",
    "# Class indices for person, car, truck, bus, and motorcycle in COCO dataset\n",
    "TARGET_CLASSES = [0, 2, 7, 5, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output\"\n",
    "gaze_processor = GazeDetectionProcessor(OUTPUT_DIR, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rec16-1_trimmed:   1%|          | 10/1396 [00:00<00:42, 32.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistent gaze circle radius: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rec16-1_trimmed: : 2792it [00:26, 103.64it/s]                        \n"
     ]
    }
   ],
   "source": [
    "video_config = [\n",
    "    {\n",
    "        \"video_id\": \"Rec16-1_trimmed\",\n",
    "        \"input_path\": INPUT_VIDEO_PATH,\n",
    "        \"timestamps_path\": INPUT_TIMESTAMP_PATH\n",
    "    }\n",
    "]\n",
    "\n",
    "gaze_processor.process_videos(video_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
