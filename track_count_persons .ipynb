{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30beac26",
   "metadata": {},
   "source": [
    "# Detect, track and count Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac57944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "import colorsys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e86c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['person', 'bicycle', 'car', 'motorcycle',\n",
    "               'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "               'traffic light', 'fire hydrant', 'stop sign',\n",
    "               'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "               'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "               'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "               'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "               'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
    "               'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "               'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
    "               'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "               'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
    "               'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "               'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "               'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "               'toothbrush']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = \"data/images/image.png\"\n",
    "# results = model(IMAGE_PATH, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in results:\n",
    "#     boxes = result.boxes  # Boxes object for bbox outputs\n",
    "#     probs = result.probs  # Class probabilities for classification outputs\n",
    "#     cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "#     xyxy = boxes.xyxy\n",
    "#     xywh = boxes.xywh  # box with xywh format, (N, 4)\n",
    "#     conf = boxes.conf\n",
    "#     print(cls)\n",
    "#     for class_index in cls:\n",
    "#         class_name = class_names[int(class_index)]\n",
    "#         print(\"Class:\", class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c7b6e",
   "metadata": {},
   "source": [
    "# DeepSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"data/videos/Rec16-1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = 'output.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee871331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames(cap, frame_queue, max_queue_size):\n",
    "    while True:\n",
    "        if frame_queue.qsize() < max_queue_size:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_queue.put(frame)\n",
    "        else:\n",
    "            time.sleep(0.1)  # Sleep briefly to prevent busy-waiting\n",
    "    frame_queue.put(None)  # Signal end of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd784b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "def process_frames(frame_queue, result_queue, model):\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model(og_frame, device=0, conf=CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        result_queue.put((og_frame, results))\n",
    "    result_queue.put(None)  # Signal end of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_and_visualize(result_queue, output_queue, tracker, class_names):\n",
    "    unique_track_ids = set()\n",
    "    while True:\n",
    "        item = result_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        \n",
    "        og_frame, results = item\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            # No detections in this frame\n",
    "            output_queue.put(og_frame)\n",
    "            continue\n",
    "        \n",
    "        result = results[0]  # Assuming single image input\n",
    "        boxes = result.boxes\n",
    "        cls = boxes.cls.tolist()\n",
    "        xyxy = boxes.xyxy\n",
    "        conf = boxes.conf\n",
    "        xywh = boxes.xywh\n",
    "        \n",
    "        if len(cls) == 0:\n",
    "            # No classes detected in this frame\n",
    "            output_queue.put(og_frame)\n",
    "            continue\n",
    "        \n",
    "        pred_cls = np.array(cls)\n",
    "        conf = conf.detach().cpu().numpy()\n",
    "        xyxy = xyxy.detach().cpu().numpy()\n",
    "        bboxes_xywh = xywh.cpu().numpy()\n",
    "        \n",
    "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            x1, y1, x2, y2 = track.to_tlbr()\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            color_id = track_id % 3\n",
    "            color = [(0, 0, 255), (255, 0, 0), (0, 255, 0)][color_id]\n",
    "            \n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "            \n",
    "            # Safely get class name\n",
    "            class_index = int(cls[track_id % len(cls)]) if cls else 0\n",
    "            class_name = class_names[class_index] if class_index < len(class_names) else \"Unknown\"\n",
    "            \n",
    "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            unique_track_ids.add(track_id)\n",
    "        \n",
    "        person_count = len(unique_track_ids)\n",
    "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        output_queue.put(og_frame)\n",
    "    output_queue.put(None)  # Signal end of tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c30f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(output_queue, out):\n",
    "    while True:\n",
    "        frame = output_queue.get()\n",
    "        if frame is None:\n",
    "            break\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_queue = Queue(maxsize=30)\n",
    "result_queue = Queue(maxsize=30)\n",
    "output_queue = Queue(maxsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_thread = Thread(target=read_frames, args=(cap, frame_queue, 30))\n",
    "process_thread = Thread(target=process_frames, args=(frame_queue, result_queue, model))\n",
    "track_thread = Thread(target=track_and_visualize, args=(result_queue, output_queue, tracker, class_names))\n",
    "write_thread = Thread(target=write_video, args=(output_queue, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef480bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_thread.start()\n",
    "# process_thread.start()\n",
    "# track_thread.start()\n",
    "# write_thread.start()\n",
    "\n",
    "# read_thread.join()\n",
    "# process_thread.join()\n",
    "# track_thread.join()\n",
    "# write_thread.join()\n",
    "\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ff5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = []\n",
    "\n",
    "# unique_track_ids = set()\n",
    "\n",
    "# i = 0\n",
    "# counter, fps, elapsed = 0, 0, 0\n",
    "# start_time = time.perf_counter()\n",
    "\n",
    "# while cap.isOpened() and i < 300:\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     if ret:\n",
    "        \n",
    "#         og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         frame = og_frame.copy()\n",
    "\n",
    "#         model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "#         results = model(frame, device=0, conf=0.5)\n",
    "\n",
    "#         class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "#         for result in results:\n",
    "#             boxes = result.boxes  # Boxes object for bbox outputs\n",
    "#             probs = result.probs  # Class probabilities for classification outputs\n",
    "#             cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "#             xyxy = boxes.xyxy\n",
    "#             conf = boxes.conf\n",
    "#             xywh = boxes.xywh  # box with xywh format, (N, 4)\n",
    "#             for class_index in cls:\n",
    "#                 class_name = class_names[int(class_index)]\n",
    "#                 #print(\"Class:\", class_name)\n",
    "\n",
    "#         pred_cls = np.array(cls)\n",
    "#         conf = conf.detach().cpu().numpy()\n",
    "#         xyxy = xyxy.detach().cpu().numpy()\n",
    "#         bboxes_xywh = xywh\n",
    "#         bboxes_xywh = xywh.cpu().numpy()\n",
    "#         bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n",
    "        \n",
    "#         tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        \n",
    "#         for track in tracker.tracker.tracks:\n",
    "#             track_id = track.track_id\n",
    "#             hits = track.hits\n",
    "#             x1, y1, x2, y2 = track.to_tlbr()  # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
    "#             w = x2 - x1  # Calculate width\n",
    "#             h = y2 - y1  # Calculate height\n",
    "\n",
    "#             # Set color values for red, blue, and green\n",
    "#             red_color = (0, 0, 255)  # (B, G, R)\n",
    "#             blue_color = (255, 0, 0)  # (B, G, R)\n",
    "#             green_color = (0, 255, 0)  # (B, G, R)\n",
    "\n",
    "#             # Determine color based on track_id\n",
    "#             color_id = track_id % 3\n",
    "#             if color_id == 0:\n",
    "#                 color = red_color\n",
    "#             elif color_id == 1:\n",
    "#                 color = blue_color\n",
    "#             else:\n",
    "#                 color = green_color\n",
    "\n",
    "#             cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "#             text_color = (0, 0, 0)  # Black color for text\n",
    "#             cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n",
    "\n",
    "#             # Add the track_id to the set of unique track IDs\n",
    "#             unique_track_ids.add(track_id)\n",
    "\n",
    "#         # Update the person count based on the number of unique track IDs\n",
    "#         person_count = len(unique_track_ids)\n",
    "\n",
    "#         # Update FPS and place on frame\n",
    "#         current_time = time.perf_counter()\n",
    "#         elapsed = (current_time - start_time)\n",
    "#         counter += 1\n",
    "#         if elapsed > 1:\n",
    "#             fps = counter / elapsed\n",
    "#             counter = 0\n",
    "#             start_time = current_time\n",
    "        \n",
    "#         i += 1\n",
    "\n",
    "#         # Draw person count on frame\n",
    "#         cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#         # Append the frame to the list\n",
    "#         frames.append(og_frame)\n",
    "\n",
    "#         # Write the frame to the output video file\n",
    "#         out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "#         # Show the frame\n",
    "#         #cv2.imshow(\"Video\", og_frame)\n",
    "# #         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "# #             break\n",
    "\n",
    "# cap.release()\n",
    "# out.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2d260",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eea58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_read_frames(yolo_cap, yolo_frame_queue, yolo_max_queue_size):\n",
    "    while True:\n",
    "        if yolo_frame_queue.qsize() < yolo_max_queue_size:\n",
    "            ret, frame = yolo_cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            yolo_frame_queue.put(frame)\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "    yolo_frame_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de72139",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "def yolo_process_frames(yolo_frame_queue, yolo_result_queue, yolo_model):\n",
    "    # Class indices for person, car, truck, bus, and motorcycle in COCO dataset\n",
    "    yolo_target_classes = [0, 2, 7, 5, 3]\n",
    "    \n",
    "    while True:\n",
    "        yolo_frame = yolo_frame_queue.get()\n",
    "        if yolo_frame is None:\n",
    "            break\n",
    "        \n",
    "        # Run YOLOv8 inference with specific classes and confidence threshold\n",
    "        yolo_results = yolo_model(yolo_frame, classes=yolo_target_classes, conf=YOLO_CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        yolo_result_queue.put((yolo_frame, yolo_results))\n",
    "    \n",
    "    yolo_result_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_write_video(yolo_result_queue, yolo_out):\n",
    "    while True:\n",
    "        yolo_item = yolo_result_queue.get()\n",
    "        if yolo_item is None:\n",
    "            break\n",
    "        yolo_frame, yolo_results = yolo_item\n",
    "        yolo_annotated_frame = yolo_results[0].plot()\n",
    "        yolo_out.write(yolo_annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60134e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_process_video(yolo_input_path, yolo_output_path, yolo_model_path=\"yolov8n.pt\"):\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    yolo_cap = cv2.VideoCapture(yolo_input_path)\n",
    "    yolo_width = int(yolo_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    yolo_height = int(yolo_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    yolo_fps = int(yolo_cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    yolo_fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    yolo_out = cv2.VideoWriter(yolo_output_path, yolo_fourcc, yolo_fps, (yolo_width, yolo_height))\n",
    "\n",
    "    yolo_frame_queue = Queue(maxsize=30)\n",
    "    yolo_result_queue = Queue(maxsize=30)\n",
    "\n",
    "    yolo_read_thread = Thread(target=yolo_read_frames, args=(yolo_cap, yolo_frame_queue, 30))\n",
    "    yolo_process_thread = Thread(target=yolo_process_frames, args=(yolo_frame_queue, yolo_result_queue, yolo_model))\n",
    "    yolo_write_thread = Thread(target=yolo_write_video, args=(yolo_result_queue, yolo_out))\n",
    "\n",
    "    yolo_read_thread.start()\n",
    "    yolo_process_thread.start()\n",
    "    yolo_write_thread.start()\n",
    "\n",
    "    yolo_read_thread.join()\n",
    "    yolo_process_thread.join()\n",
    "    yolo_write_thread.join()\n",
    "\n",
    "    yolo_cap.release()\n",
    "    yolo_out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_input_video = \"data/videos/Rec16-1.mp4\"\n",
    "yolo_output_video = \"output/yolo_output.mp4\"\n",
    "yolo_process_video(yolo_input_video, yolo_output_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
