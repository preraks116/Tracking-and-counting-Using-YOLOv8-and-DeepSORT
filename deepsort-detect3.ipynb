{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "from deep_sort.sort.tracker import Tracker\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['person', 'bicycle', 'car', 'motorcycle',\n",
    "               'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "               'traffic light', 'fire hydrant', 'stop sign',\n",
    "               'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "               'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "               'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "               'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "               'skateboard', 'surfboard', 'tennis racket', 'bottle',\n",
    "               'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "               'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
    "               'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "               'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
    "         'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "               'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "               'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "               'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUEUE_SIZE = 30\n",
    "SLEEP_TIME = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_MODEL_PATH = \"yolov8n.pt\"\n",
    "YOLO_CONFIDENCE_THRESHOLD = 0.5\n",
    "TARGET_CLASSES = [0, 2, 7, 5, 3]  # person, car, truck, bus, motorcycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_SORT_MODEL_PATH = \"deep_sort/deep/checkpoint/ckpt.t7\"\n",
    "DEEP_SORT_MAX_AGE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"data/videos/Rec16-1_trimmed.mp4\"\n",
    "OUTPUT_PATH = \"output/videos/Rec16-1_trimmed_deepsort.mp4\"\n",
    "JSON_OUTPUT_PATH = \"output/json/Rec16-1_trimmed_deepsort.json\"\n",
    "INPUT_TIMESTAMP_PATH = \"output/timestamps/Rec16-1_trimmed.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMESTAMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timestamps(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEPSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames(cap, frame_queue, max_queue_size):\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        while True:\n",
    "            if frame_queue.qsize() < max_queue_size:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame_queue.put(frame)\n",
    "            else:\n",
    "                time.sleep(SLEEP_TIME)  # Sleep briefly to prevent busy-waiting\n",
    "    frame_queue.put(None)  # Signal end of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(frame_queue, result_queue, model):\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model(og_frame, device=0, classes=TARGET_CLASSES, conf=YOLO_CONFIDENCE_THRESHOLD)\n",
    "        \n",
    "        result_queue.put((og_frame, results))\n",
    "    result_queue.put(None)  # Signal end of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_and_visualize(result_queue, output_queue, tracker, class_names, timestamps):\n",
    "    unique_track_ids = set()\n",
    "    frame_index = 0\n",
    "    results_dict = {}\n",
    "    \n",
    "    while True:\n",
    "        item = result_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        \n",
    "        og_frame, results = item\n",
    "        \n",
    "        timestamp = timestamps[frame_index] if frame_index < len(timestamps) else f\"frame_{frame_index}\"\n",
    "        frame_results = []\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            # No detections in this frame\n",
    "            results_dict[timestamp] = frame_results\n",
    "            output_queue.put(og_frame)\n",
    "            frame_index += 1\n",
    "            continue\n",
    "        \n",
    "        result = results[0]  # Assuming single image input\n",
    "        boxes = result.boxes\n",
    "        cls = boxes.cls.tolist()\n",
    "        xyxy = boxes.xyxy\n",
    "        conf = boxes.conf\n",
    "        xywh = boxes.xywh\n",
    "        \n",
    "        if len(cls) == 0:\n",
    "            # No classes detected in this frame\n",
    "            results_dict[timestamp] = frame_results\n",
    "            output_queue.put(og_frame)\n",
    "            frame_index += 1\n",
    "            continue\n",
    "        \n",
    "        pred_cls = np.array(cls)\n",
    "        conf = conf.detach().cpu().numpy()\n",
    "        xyxy = xyxy.detach().cpu().numpy()\n",
    "        bboxes_xywh = xywh.cpu().numpy()\n",
    "        \n",
    "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
    "        for track in tracker.tracker.tracks:\n",
    "            track_id = track.track_id\n",
    "            x1, y1, x2, y2 = track.to_tlbr()\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            \n",
    "            color_id = track_id % 3\n",
    "            color = [(0, 0, 255), (255, 0, 0), (0, 255, 0)][color_id]\n",
    "            \n",
    "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "            \n",
    "            # Safely get class name\n",
    "            class_index = int(cls[track_id % len(cls)]) if cls else 0\n",
    "            class_name = class_names[class_index] if class_index < len(class_names) else \"Unknown\"\n",
    "            \n",
    "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "            unique_track_ids.add(track_id)\n",
    "            \n",
    "            # Store detection and tracking information\n",
    "            frame_results.append({\n",
    "                \"track_id\": track_id,\n",
    "                \"class\": class_name,\n",
    "                \"bbox\": [x1, y1, x2, y2],\n",
    "                \"confidence\": float(conf[track_id % len(conf)])  # Safely get confidence\n",
    "            })\n",
    "        \n",
    "        results_dict[timestamp] = frame_results\n",
    "        output_queue.put(og_frame)\n",
    "        frame_index += 1\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    with open(JSON_OUTPUT_PATH, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    \n",
    "    output_queue.put(None)  # Signal end of tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(output_queue, out):\n",
    "    while True:\n",
    "        frame = output_queue.get()\n",
    "        if frame is None:\n",
    "            break\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, timestamps_path, yolo_model_path=YOLO_MODEL_PATH):\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    tracker = DeepSort(model_path=DEEP_SORT_MODEL_PATH, max_age=DEEP_SORT_MAX_AGE)\n",
    "    timestamps = load_timestamps(timestamps_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    assert len(timestamps) == total_frames, \"Number of timestamps must match number of frames in video\"\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_queue = Queue(maxsize=MAX_QUEUE_SIZE)\n",
    "    result_queue = Queue(maxsize=MAX_QUEUE_SIZE)\n",
    "    output_queue = Queue(maxsize=MAX_QUEUE_SIZE)\n",
    "    \n",
    "    read_thread = Thread(target=read_frames, args=(cap, frame_queue, MAX_QUEUE_SIZE))\n",
    "    process_thread = Thread(target=process_frames, args=(frame_queue, result_queue, yolo_model))\n",
    "    track_thread = Thread(target=track_and_visualize, args=(result_queue, output_queue, tracker, class_names, timestamps))\n",
    "    write_thread = Thread(target=write_video, args=(output_queue, out))\n",
    "    \n",
    "    read_thread.start()\n",
    "    process_thread.start()\n",
    "    track_thread.start()\n",
    "    write_thread.start()\n",
    "\n",
    "    read_thread.join()\n",
    "    process_thread.join()\n",
    "    track_thread.join()\n",
    "    write_thread.join()\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(VIDEO_PATH, OUTPUT_PATH, INPUT_TIMESTAMP_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
